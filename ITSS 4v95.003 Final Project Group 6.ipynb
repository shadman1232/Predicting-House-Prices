{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"Return\"></a>\n",
    "# Table of Contents\n",
    "\n",
    "* ## [Data Description](#Data_Description)\n",
    "   #### * [Overview of Dataset](#overview)\n",
    "   #### * [Exploratory Data Analysis](#eda)   \n",
    "* ## [Data Cleansing/Pre-Processing](#Data_Cleansing)\n",
    "   #### * [Imputing Missing Values-Training](#impute_missing_train)\n",
    "   #### * [Imputing Missing Values-Testing](#impute_missing_test)\n",
    "   #### * [Converting columns of categorical type to numerical type](#conv_columns)\n",
    "* ## [Finding Most Important Features](#best_features)\n",
    "   ####  * [SelectKBest](#SelectKBest)\n",
    "   ####  * [ExtraTreesClassifier](#ExtraTreesClassifier)\n",
    "* ## [Regression Techniques](#RegTech)\n",
    "   ####  * [Linear Regression](#linReg)\n",
    "   ####  * [Boosted Trees Regression](#bReg)\n",
    "* ## [Classification Techniques](#class)\n",
    "   ####  * [Creating Bins](#Bins)\n",
    "   ####  * [Create Generic Model](#GenericClass)\n",
    "   ####  * [Create Random Forest Classification Model](#RandomForest)\n",
    "* ## [Graphs](#graphs)\n",
    "   ####  * [Violin Plots-Regression](#Violin_PlotsR)\n",
    "   ####  * [Violin Plots-Classification](#Violin_PlotsC)\n",
    "   #### * [Histogram-Regression](#RHistogram)\n",
    "   #### * [Histogram-Classification](#CHistogram)\n",
    "   ####  * [Line Plots-Regression](#Line_PlotsR)\n",
    "* ## [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages\n",
    "import pandas as pd # Used for cleaning the data and filling in missing values\n",
    "import numpy as np\n",
    "import turicreate as tc\n",
    "from sklearn.feature_selection import SelectKBest # Used for finding the best combination of features\n",
    "from sklearn.feature_selection import chi2 # Used for finding the best combination of features\n",
    "from sklearn.ensemble import ExtraTreesClassifier # Used for finding the best combination of features\n",
    "import matplotlib.pyplot as plt # Used for visualizations\n",
    "import itertools as it # Used for finding the best combination of features\n",
    "import seaborn as sns # Used for visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"Data_Description\"></a>\n",
    "## Data Description\n",
    "\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in both datasets\n",
    "train_df = pd.read_csv(r'train.csv', index_col = 0)\n",
    "test_df = pd.read_csv(r'test.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"overview\"></a>\n",
    "### Overview of Dataset\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing # of rows and columns in both datasets\n",
    "print('Training data:', train_df.shape)\n",
    "print('Testing data', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data types of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out data types of train dataset\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out # of features for each data type in train dataset\n",
    "train_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out data types of test dataset\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out # of features for each data type in test dataset\n",
    "test_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"eda\"></a>\n",
    "### Exploratory Data Analysis\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out first 5 rows of train dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out first 5 rows of test dataset\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out last 5 rows of train dataset\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out last 5 rows of test dataset\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out random 5 rows of train dataset\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out random 5 rows of test dataset\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out concise summary of columns, their data types, and non-null values in train dataset\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out concise summary of columns, their data types, and non-null values in test dataset\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out descriptive statistics of train dataset\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out descriptive statistics of test dataset\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"Data_Cleansing\"></a>\n",
    "## Data Cleansing/Pre-Processing\n",
    "\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"impute_missing_train\"></a>\n",
    "### Imputing Missing Values in Training Dataset\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns with NA values and the number of NA values in these columns in the training data\n",
    "columns_with_detected_null_values_in_train_data = train_df.columns[train_df.isna().any()].tolist()\n",
    "train_df[columns_with_detected_null_values_in_train_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns with NA values and the number of NA values in these columns in the testing data\n",
    "columns_with_detected_null_values_in_test_data = test_df.columns[test_df.isna().any()].tolist()\n",
    "test_df[columns_with_detected_null_values_in_test_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are columns that are considered to have null/na values in Pandas. However, in the data dictionary \n",
    "# na simply means that there is none. For example, some houses may or may not have an alley, but Pandas will interpret \n",
    "# these NA values as being empty data points, which we don't want. The next three cells solve this problem for the training\n",
    "# data\n",
    "\n",
    "columns_with_no_actual_null_values = ['Alley',\n",
    " 'BsmtQual',\n",
    " 'BsmtCond',\n",
    " 'BsmtExposure',\n",
    " 'BsmtFinType1',\n",
    " 'BsmtFinType2',\n",
    " 'FireplaceQu',\n",
    " 'GarageType',\n",
    " 'GarageFinish',\n",
    " 'GarageQual',\n",
    " 'GarageCond',\n",
    " 'PoolQC',\n",
    " 'Fence',\n",
    " 'MiscFeature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will iterate through the list of columns_with_no_actual_null values and replace the NA values with a placeholder value\n",
    "for col in columns_with_no_actual_null_values:\n",
    "    train_df[col].fillna('asdf', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whenever we run this cell, we store any remaining columns in the dataframe that still have NA values\n",
    "# into this variable, This is done after every time we fill in the NA values to see which columns still have NA values\n",
    "columns_with_detected_null_values_in_train_data = train_df.columns[train_df.isna().any()].tolist()\n",
    "train_df[columns_with_detected_null_values_in_train_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datatypes of remaining null values\n",
    "train_df[columns_with_detected_null_values_in_train_data].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any of the remaining columns with null values are an object type, we will fill those with the value that occurs most\n",
    "# often in that column\n",
    "for col in columns_with_detected_null_values_in_train_data:\n",
    "    if train_df[col].dtypes == 'object':\n",
    "        train_df[col].fillna((train_df[col].value_counts()[0]), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present \n",
    "columns_with_detected_null_values_in_train_data = train_df.columns[train_df.isna().any()].tolist()\n",
    "train_df[columns_with_detected_null_values_in_train_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in NA values with the mean\n",
    "train_df['LotFrontage'].fillna((train_df['LotFrontage'].mean()), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present\n",
    "columns_with_detected_null_values_in_train_data = train_df.columns[train_df.isna().any()].tolist()\n",
    "train_df[columns_with_detected_null_values_in_train_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in NA values with the median\n",
    "train_df['MasVnrArea'].fillna((train_df['MasVnrArea'].median()), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present\n",
    "columns_with_detected_null_values_in_train_data = train_df.columns[train_df.isna().any()].tolist()\n",
    "train_df[columns_with_detected_null_values_in_train_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in NA values with the median\n",
    "train_df['GarageYrBlt'].fillna((train_df['GarageYrBlt'].median()), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present. In this case, all columns are clean and \n",
    "# do not contain any NA values\n",
    "columns_with_detected_null_values_in_train_data = train_df.columns[train_df.isna().any()].tolist()\n",
    "train_df[columns_with_detected_null_values_in_train_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"impute_missing_test\"></a>\n",
    "### Imputing Missing Values In Test Dataset\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present\n",
    "columns_with_detected_null_values_in_test_data = test_df.columns[test_df.isna().any()].tolist()\n",
    "test_df[columns_with_detected_null_values_in_test_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_no_actual_null_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will iterate through the list of columns_with_no_actual_null values and replace the NA values with a placeholder value\n",
    "for col in columns_with_no_actual_null_values:\n",
    "    test_df[col].fillna('asdf', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present\n",
    "columns_with_detected_null_values_in_test_data = test_df.columns[test_df.isna().any()].tolist()\n",
    "test_df[columns_with_detected_null_values_in_test_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datatypes of remaining null values\n",
    "test_df[columns_with_detected_null_values_in_test_data].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any of the remaining columns with null values are an object type, we will fill those with the value that occurs most\n",
    "# often in that column\n",
    "for col in columns_with_detected_null_values_in_test_data:\n",
    "    if test_df[col].dtypes == 'object':\n",
    "        test_df[col].fillna((test_df[col].value_counts()[0]), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present\n",
    "columns_with_detected_null_values_in_test_data = test_df.columns[test_df.isna().any()].tolist()\n",
    "test_df[columns_with_detected_null_values_in_test_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in NA values with the mean\n",
    "test_df['LotFrontage'].fillna((test_df['LotFrontage'].mean()), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present\n",
    "columns_with_detected_null_values_in_test_data = test_df.columns[test_df.isna().any()].tolist()\n",
    "test_df[columns_with_detected_null_values_in_test_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in NA values with the median\n",
    "test_df['MasVnrArea'].fillna((test_df['MasVnrArea'].median()), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present\n",
    "columns_with_detected_null_values_in_test_data = test_df.columns[test_df.isna().any()].tolist()\n",
    "test_df[columns_with_detected_null_values_in_test_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each column in this list was filled in with the most common values in said columns\n",
    "for col in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageCars']:\n",
    "    test_df[col].fillna((test_df[col].value_counts()[0]), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present\n",
    "columns_with_detected_null_values_in_test_data = test_df.columns[test_df.isna().any()].tolist()\n",
    "test_df[columns_with_detected_null_values_in_test_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in NA values with the median\n",
    "test_df['GarageYrBlt'].fillna((test_df['GarageYrBlt'].median()), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present\n",
    "columns_with_detected_null_values_in_test_data = test_df.columns[test_df.isna().any()].tolist()\n",
    "test_df[columns_with_detected_null_values_in_test_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in NA values with the mean\n",
    "test_df['TotalBsmtSF'].fillna((test_df['TotalBsmtSF'].mean()), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present\n",
    "columns_with_detected_null_values_in_test_data = test_df.columns[test_df.isna().any()].tolist()\n",
    "test_df[columns_with_detected_null_values_in_test_data].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in NA values with the mean\n",
    "test_df['GarageArea'].fillna((test_df['GarageArea'].mean()), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refreshing variable with list of columns where there are still NA values present, which in this case is none\n",
    "columns_with_detected_null_values_in_test_data = test_df.columns[test_df.isna().any()].tolist()\n",
    "test_df[columns_with_detected_null_values_in_test_data].isnull().sum()\n",
    "# At this point, there are no longer any null values in both our training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"conv_columns\"></a>\n",
    "### Converting columns of categorical type to numerical type\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = train_df.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object to object dtype to category \n",
    "for col in cat_columns:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all category columns to integer (label encoding)\n",
    "train_df[cat_columns] = train_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "test_df[cat_columns] = test_df[cat_columns].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"best_features\"></a>\n",
    "## Finding most important features\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"SelectKBest\"></a>\n",
    "### Method 1: SelectKBest\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop('SalePrice', axis = 1) # Independent features/columns\n",
    "y = train_df['SalePrice'] # Target feature/column\n",
    "\n",
    "# Using SelectKBest class with chi-squared test (used to measure statistical significance) to retrieve the top 10 features with highest importance scores\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10) \n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  \n",
    "print(featureScores.nlargest(10,'Score'))  # Printing the 10 best features\n",
    "\n",
    "first_set_of_important_features = list(featureScores.nlargest(10,'Score')['Specs']) # Retrieving the 10 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"ExtraTreesClassifier\"></a>\n",
    "### Method 2: ExtraTreesClassifier\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns) # Using inbuilt class feature_importances of tree based classifiers\n",
    "feat_importances.nlargest(10).plot(kind='barh') # Plotting bar chart of feature importance\n",
    "plt.title('Method 2 - Top 10 features with highest importance scores')\n",
    "plt.xlabel('Feature importance score')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "second_set_of_important_features = list(feat_importances.nlargest(10).index) # Retrieving features that have the 10 highest feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a cumulative list of unique, important features from the 2 ways\n",
    "\n",
    "cumulative_list_important_features = first_set_of_important_features + second_set_of_important_features\n",
    "cumulative_set_important_features = set(cumulative_list_important_features)\n",
    "print(\"Cumulative set of unique, important features from the two ways:\", cumulative_set_important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"RegTech\"></a>\n",
    "## Regression Techniques\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"linReg\"></a>\n",
    "### Linear Regression\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating generic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting DataFrames to SFrames for Turicreate models to run on\n",
    "train_SF_regression = tc.SFrame(train_df)\n",
    "test_SF_regression = tc.SFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating generic regression model\n",
    "regression_model = tc.regression.create(train_SF_regression, target='SalePrice')\n",
    "regression_model_errors = regression_model.evaluate(test_SF_regression)\n",
    "print(regression_model_errors) # Printing out errors after testing evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating linear regression model and finding the ideal combination of features to generate least amount of RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_linear_regression_model_features_and_errors = []\n",
    "\n",
    "for i in range(1, 4): # Choosing combinations from 1 to 3 features inclusive due to high computation expense\n",
    "    list_combination_indexes = list(it.combinations(list(cumulative_set_important_features), i))\n",
    "    ideal_linear_regression_model_features = list(list_combination_indexes[0])\n",
    "    linear_regression_model = tc.linear_regression.create(train_SF_regression, target = 'SalePrice', features = ideal_linear_regression_model_features)\n",
    "    ideal_linear_regression_model_errors = linear_regression_model.evaluate(test_SF_regression)\n",
    "    \n",
    "    print('BEGINNING')\n",
    "    print('Number of features used:', len(ideal_linear_regression_model_features))\n",
    "    print('Features used:', ideal_linear_regression_model_features)\n",
    "    print('Errors:', ideal_linear_regression_model_errors)\n",
    "\n",
    "    for j in list_combination_indexes: # iterating through the items in list of combinations of size i\n",
    "        linear_regression_model = tc.linear_regression.create(train_SF_regression, target = 'SalePrice', features = list(j))\n",
    "        linear_regression_model_errors = linear_regression_model.evaluate(test_SF_regression)\n",
    "        \n",
    "        if (linear_regression_model_errors['rmse'] < ideal_linear_regression_model_errors['rmse']): # comparing the error with current Ideal\n",
    "            ideal_linear_regression_model_features = list(j) # Finding ideal features for each number combination with lowest amount of RMSE\n",
    "            ideal_linear_regression_model_errors = linear_regression_model_errors\n",
    "    \n",
    "    print('ENDING')\n",
    "    print('Number of features used:', len(ideal_linear_regression_model_features))\n",
    "    print('Features used:', ideal_linear_regression_model_features)\n",
    "    print('Errors:', ideal_linear_regression_model_errors)\n",
    "    \n",
    "    ideal_linear_regression_model_features_and_errors.append([ideal_linear_regression_model_features, ideal_linear_regression_model_errors['rmse']])\n",
    "\n",
    "print(ideal_linear_regression_model_features_and_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ideal_linear_regression_model_features_and_errors = ideal_linear_regression_model_features_and_errors[0]\n",
    "\n",
    "for i in ideal_linear_regression_model_features_and_errors: # Finding ideal combination of features out of the ideal features for each number combination\n",
    "    if i[1] < final_ideal_linear_regression_model_features_and_errors[1]:\n",
    "        final_ideal_linear_regression_model_features_and_errors = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing results\n",
    "print(\"Ideal number of features used:\", len(final_ideal_linear_regression_model_features_and_errors[0]))\n",
    "print(\"Ideal combination of features used:\", final_ideal_linear_regression_model_features_and_errors[0])\n",
    "print(\"RMSE:\", final_ideal_linear_regression_model_features_and_errors[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"bReg\"></a>\n",
    "### Boosted Regression Tree\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating boosted regression tree model and finding ideal max_depth and # of max_iterations to generate least amount of RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Testing the number of parameters only up to 10 due to expensive computation\n",
    "a = list(range(1, 11))\n",
    "b = list(range(1, 11))\n",
    "c = list(it.product(a, b))\n",
    "print(c)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for val in c:\n",
    "    x.append(val[0])\n",
    "    y.append(val[1])\n",
    "\n",
    "z = []\n",
    "\n",
    "ideal_max_iterations_boosted_trees_regression = 1\n",
    "ideal_max_depth_boosted_trees_regression = 1\n",
    "\n",
    "ideal_boosted_trees_regression_model = tc.boosted_trees_regression.create(train_SF_regression, target = 'SalePrice', features = list(cumulative_set_important_features), max_iterations = 1, max_depth = 1)\n",
    "ideal_boosted_trees_regression_model_errors = ideal_boosted_trees_regression_model.evaluate(test_SF_regression)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    for j in range(1, 11):\n",
    "        test_boosted_trees_regression_model = tc.boosted_trees_regression.create(train_SF_regression, target = 'SalePrice', features = list(cumulative_set_important_features), max_iterations = i, max_depth = j)\n",
    "        test_boosted_trees_regression_model_errors = test_boosted_trees_regression_model.evaluate(test_SF_regression)\n",
    "        \n",
    "        z.append(test_boosted_trees_regression_model_errors['rmse'])\n",
    "\n",
    "        if test_boosted_trees_regression_model_errors['rmse'] < ideal_boosted_trees_regression_model_errors['rmse']:\n",
    "            ideal_max_iterations_boosted_trees_regression = i\n",
    "            ideal_max_depth_boosted_trees_regression = j\n",
    "            ideal_boosted_trees_regression_model_errors['rmse'] = test_boosted_trees_regression_model_errors['rmse']\n",
    "\n",
    "df = pd.DataFrame({'max_iterations':x, 'max_depth':y, 'rmse':z})\n",
    "\n",
    "# Creating a heatmap to show different combinations of max_depth and max_iterations and the model's corresponding RMSE\n",
    "sns.heatmap(pd.crosstab(df['max_iterations'], df['max_depth'], values=df['rmse'] / 10000, aggfunc='sum'), linewidths=.5, ax=ax,annot=True)\n",
    "\n",
    "ax.set_title('Ideal # of max_iterations and max_depth (RMSE shown in 10,000s) - Boosted trees regression model', fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing ideal max_iterations and max_depth for the boosted regression tree model to generate the least amount of RMSE\n",
    "print(ideal_max_iterations_boosted_trees_regression)\n",
    "print(ideal_max_depth_boosted_trees_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"class\"></a>\n",
    "## Classification Techniques\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"Bins\"></a>\n",
    "### Creating Bins\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum range of SalePrice between the testing & training data\n",
    "max_value = max(train_df['SalePrice'])\n",
    "min_value = min(train_df['SalePrice'])\n",
    "\n",
    "if max(test_df['SalePrice']) > max_value:\n",
    "    max_value = max(test_df['SalePrice'])\n",
    "\n",
    "if min(test_df['SalePrice']) < min_value:\n",
    "    min_value = min(test_df['SalePrice'])\n",
    "\n",
    "# Creating the bins and their respective labels which will be used to encode the new target column, \n",
    "# 'SalePrice_label_for_bin_range'\n",
    "\n",
    "bins = []\n",
    "\n",
    "for i in range(min_value - 10000, max_value + 10001, 10000):\n",
    "    bins.append(i)\n",
    "\n",
    "labels = list(range(1, len(bins)))\n",
    "## Train\n",
    "# Creating new target column which is in the format of just an integer\n",
    "train_df['SalePrice_bin_range'] = pd.cut(train_df['SalePrice'], bins = bins)\n",
    "train_df['SalePrice_label_for_bin_range'] = pd.cut(train_df['SalePrice'], bins = bins, labels = labels)\n",
    "\n",
    "# Create a dictionary to match the bin label to the actual range\n",
    "label_for_bin_range_SalePrice = dict(zip(list(train_df['SalePrice_label_for_bin_range']), list(train_df['SalePrice_bin_range'].astype(str))))\n",
    "\n",
    "## Test\n",
    "# Creating new target column which is in the format of just an integer\n",
    "test_df['SalePrice_bin_range'] = pd.cut(test_df['SalePrice'], bins = bins)\n",
    "test_df['SalePrice_label_for_bin_range'] = pd.cut(test_df['SalePrice'], bins = bins, labels = labels)\n",
    "\n",
    "# Drop bin range due to it not being necessary in our analysis\n",
    "train_df.drop('SalePrice_bin_range', inplace = True, axis = 1)\n",
    "test_df.drop('SalePrice_bin_range', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['SalePrice_label_for_bin_range'] = train_df['SalePrice_label_for_bin_range'].astype(int)\n",
    "test_df['SalePrice_label_for_bin_range'] = test_df['SalePrice_label_for_bin_range'].astype(int)\n",
    "\n",
    "# Creating the new SFrame by dropping SalePrice so that it is not used as a feature to predict the 'SalePrice_label_for_bin_range'\n",
    "\n",
    "train_df_classifier = train_df.drop('SalePrice', axis = 1)\n",
    "test_df_classifier = test_df.drop('SalePrice', axis = 1)\n",
    "\n",
    "train_SF_classifier = tc.SFrame(train_df_classifier)\n",
    "test_SF_classifier = tc.SFrame(test_df_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"GenericClass\"></a>\n",
    "### Creating generic classification model\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating generic classification model\n",
    "classifier_model = tc.classifier.create(train_SF_classifier, target = 'SalePrice_label_for_bin_range')\n",
    "classifier_model_errors = classifier_model.evaluate(test_SF_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out accuracy of generic model\n",
    "classifier_model_errors['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"RandomForest\"></a>\n",
    "### Creating random forest classification model\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "a = list(range(1, 11))\n",
    "b = list(range(1, 11))\n",
    "c = list(it.product(a, b))\n",
    "print(c)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for val in c:\n",
    "    x.append(val[0])\n",
    "    y.append(val[1])\n",
    "\n",
    "z = []\n",
    "\n",
    "ideal_max_iterations_random_forest_classifier = 1\n",
    "ideal_max_depth_random_forest_classifier = 1\n",
    "\n",
    "ideal_random_forest_classifier_model = tc.random_forest_classifier.create(train_SF_classifier, target='SalePrice_label_for_bin_range', max_iterations = 1, max_depth = 1)\n",
    "ideal_random_forest_classifier_model_errors = ideal_random_forest_classifier_model.evaluate(test_SF_classifier)['accuracy'] \n",
    "\n",
    "for i in range(1, 11):\n",
    "    for j in range(1, 11):\n",
    "        test_random_forest_classifier_model = tc.random_forest_classifier.create(train_SF_classifier, target='SalePrice_label_for_bin_range', max_iterations = i, max_depth = j)\n",
    "        test_random_forest_classifier_model_errors = test_random_forest_classifier_model.evaluate(test_SF_classifier)\n",
    "        \n",
    "        z.append(test_random_forest_classifier_model_errors['accuracy'])\n",
    "        \n",
    "        # Finding ideal max_iterations and max_depth to generate highest testing accuracy\n",
    "        if test_random_forest_classifier_model_errors['accuracy'] > ideal_random_forest_classifier_model_errors:\n",
    "            ideal_max_iterations_random_forest_classifier = i\n",
    "            ideal_max_depth_random_forest_classifier = j\n",
    "            ideal_random_forest_classifier_model_errors = test_random_forest_classifier_model_errors['accuracy']\n",
    "\n",
    "df = pd.DataFrame({'max_iterations':x, 'max_depth':y, 'accuracy':z})\n",
    "\n",
    "# Creating a heatmap to show different combinations of max_depth and max_iterations and the model's corresponding accuracy\n",
    "sns.heatmap(pd.crosstab(df['max_iterations'], df['max_depth'], values=df['accuracy'] * 100, aggfunc='sum'), linewidths=.5, ax=ax, annot=True)\n",
    "\n",
    "ax.set_title('Ideal # of max_iterations and max_depth (Accuracy shown in %) - Random forest classifier model', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing ideal max_iterations and max_depth for the random forest classifier model\n",
    "print(ideal_max_iterations_random_forest_classifier)\n",
    "print(ideal_max_depth_random_forest_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"graphs\"></a>\n",
    "## Graphs\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"Violin_PlotsR\"></a>\n",
    "### Violin Plots - Regression\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Violin Plots to show distribution of linear model predictions and Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take our best linear model predictions and store them\n",
    "ideal_linear_model = tc.linear_regression.create(train_SF_regression,features = final_ideal_linear_regression_model_features_and_errors[0], target='SalePrice')\n",
    "ideal_linear_predictions = ideal_linear_model.predict(test_SF_regression)\n",
    "\n",
    "# Also take our actual values for comparison\n",
    "regression_actual_values = test_SF_regression['SalePrice']\n",
    "\n",
    "# Take our generic model predictions (using all features) and store them\n",
    "generic_linear_predictions = regression_model.predict(test_SF_regression)\n",
    "\n",
    "# Take our best determined boosted trees regression model instance, recreate it, and store the predictions\n",
    "boosted_trees_model = tc.boosted_trees_regression.create(train_SF_regression, target = 'SalePrice', features = list(cumulative_set_important_features), max_iterations = ideal_max_iterations_boosted_trees_regression, max_depth = ideal_max_depth_boosted_trees_regression)\n",
    "boosted_trees_model_predictions = boosted_trees_model.predict(test_SF_regression)\n",
    "boosted_trees_model_errors = boosted_trees_model.evaluate(test_SF_regression)['rmse']\n",
    "# Make a list of all the plotting data\n",
    "regression_plot_data = [regression_actual_values,ideal_linear_predictions,boosted_trees_model_predictions,generic_linear_predictions]\n",
    "\n",
    "print('Ideal Linear Model:',ideal_linear_model.evaluate(test_SF_regression))\n",
    "print('Boosted Trees Model:',boosted_trees_model.evaluate(test_SF_regression))\n",
    "print('Generic Model:',regression_model.evaluate(test_SF_regression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For one unit of change in the feature variables LotArea, MoSold, and YrSold, there is a corresponding change\n",
    "# in the predictor variable SalePrice which is represented by the magnitude of the coefficients.\n",
    "ideal_linear_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a violin plot showing the actual values compared to the predicted values for our regression models.\n",
    "VLfig = plt.figure()\n",
    "VLax = VLfig.add_axes([0,0,2,1])\n",
    "\n",
    "Lxticklabels = ['Actual Values', 'Ideal Linear Regression Model','Boosted Model','Generic Model']\n",
    "\n",
    "VLax.set_xticks([1,2,3,4])\n",
    "VLax.set_xticklabels(Lxticklabels)\n",
    "\n",
    "plt.title('Regression models',fontsize=20)\n",
    "plt.xlabel('Models',fontsize=15)\n",
    "plt.ylabel('Values',fontsize=15)\n",
    "\n",
    "bp = VLax.violinplot(regression_plot_data)\n",
    "\n",
    "\"\"\"\n",
    "plt.ylim([0, 1000000])\n",
    "VLax.set_yticks(list(range(0, 1000001, 100000)))\n",
    "VLax.set_yticklabels(list(range(0, 1000001, 100000)))\n",
    "\"\"\"\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"Violin_PlotsC\"></a>\n",
    "### Violin Plots - Classification\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Violin Plots to show distribution of various classification models and Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take our generic classifier model and store the predicted values\n",
    "generic_classifier_predicted_values = classifier_model.predict(test_SF_classifier)\n",
    "\n",
    "# Store our actual values as an SArray for comparison\n",
    "classifier_actual_values = test_SF_classifier['SalePrice_label_for_bin_range']\n",
    "\n",
    "# Create a random forest model based on optimal max_depth, which is equal to 1, then store those predictions\n",
    "random_forest_classifier_model = tc.random_forest_classifier.create(train_SF_classifier, target='SalePrice_label_for_bin_range', max_iterations = ideal_max_iterations_random_forest_classifier, max_depth = ideal_max_depth_random_forest_classifier)\n",
    "random_forest_classifier_predictions = random_forest_classifier_model.predict(test_SF_classifier)\n",
    "random_forest_classifier_errors = random_forest_classifier_model.evaluate(test_SF_classifier)['accuracy']\n",
    "\n",
    "# Make a list of all the plotting data\n",
    "classification_plot_data = [classifier_actual_values,generic_classifier_predicted_values,random_forest_classifier_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_forest_classifier_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a violin plot showing the actual values compared to the predicted values for our classification models.\n",
    "VCfig = plt.figure()\n",
    "VCax = VCfig.add_axes([0,0,2,1])\n",
    "\n",
    "xticklabels = ['Actual Values', 'Generic Classifier','Random Forest Classifier']\n",
    "\n",
    "VCax.set_xticks([1,2,3])\n",
    "VCax.set_xticklabels(xticklabels)\n",
    "\n",
    "plt.title('Classification models',fontsize=20)\n",
    "plt.xlabel('Models',fontsize=15)\n",
    "plt.ylabel('Bin Values',fontsize=15)\n",
    "\n",
    "ap = VCax.violinplot(classification_plot_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"RHistogram\"></a>\n",
    "### Histograms - Regression \n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram plots for Regression models\n",
    "n_bins = 50\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, sharey=True,sharex=True, tight_layout=False)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "axs[0].hist(regression_plot_data[0], bins = n_bins)\n",
    "axs[1].hist(regression_plot_data[1], bins = n_bins)\n",
    "axs[2].hist(regression_plot_data[2], bins = n_bins)\n",
    "axs[3].hist(regression_plot_data[3], bins = n_bins)\n",
    "\n",
    "axs[0].set_ylabel('Count', fontsize = 12)\n",
    "\n",
    "for i in range(0, 4):\n",
    "    axs[i].set_xlabel('SalePrice', fontsize = 12)\n",
    "\n",
    "axs[0].set_title('Actual Data', fontsize = 15)\n",
    "axs[1].set_title('Ideal Linear Model Predictions', fontsize = 15)\n",
    "axs[2].set_title('Boosted Trees Model Predictions', fontsize = 15)\n",
    "axs[3].set_title('Generic Model Predictions', fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"CHistogram\"></a>\n",
    "### Histograms - Classification\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram plots for Classification models\n",
    "n_bins = 50\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, sharey=True,sharex=True, tight_layout=True)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "axs[0].hist(classification_plot_data[0], bins = n_bins)\n",
    "axs[1].hist(classification_plot_data[1], bins = n_bins)\n",
    "axs[2].hist(classification_plot_data[2], bins = n_bins)\n",
    "\n",
    "axs[0].set_ylabel('Count', fontsize = 12)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    axs[i].set_xlabel('SalePrice_BinLabels', fontsize = 12)\n",
    "    \n",
    "axs[0].set_title('Actual Data', fontsize = 15)\n",
    "axs[1].set_title('Generic Classifier', fontsize = 15)\n",
    "axs[2].set_title('Random Forest Model', fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"Line_PlotsR\"></a>\n",
    "### Line Plots - Regression\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparing predicted SalePrice vs actual SalePrice based on LotArea, MoSold, and YrSold independently for linear regression model\n",
    "test_df_used_for_graphs = test_df.copy()\n",
    "test_df_used_for_graphs['YrSold'] = test_df_used_for_graphs['YrSold'].astype(int)\n",
    "test_df_used_for_graphs['Linear_Regression_Predictions'] = ideal_linear_predictions\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10), nrows=1, ncols=3)\n",
    "fig.suptitle('Linear Regression Model - Predicted SalePrice vs. Actual SalePrice based on:')\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "sns.regplot(x='LotArea', y='SalePrice', data = test_df_used_for_graphs, label = 'Actual SalePrice', scatter=None, ci=None)\n",
    "sns.regplot(x='LotArea', y='Linear_Regression_Predictions', data = test_df_used_for_graphs, label = 'Predicted SalePrice', scatter=None)\n",
    "plt.title('Feature: LotArea')\n",
    "plt.ylabel('SalePrice')\n",
    "plt.legend()\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "sns.regplot(x='MoSold', y='SalePrice', data = test_df_used_for_graphs, label = 'Actual SalePrice', scatter=None, ci=None)\n",
    "sns.regplot(x='MoSold', y='Predictions', data = test_df_used_for_graphs, label = 'Predicted SalePrice', scatter=None)\n",
    "plt.legend()\n",
    "plt.title('Feature: MoSold')\n",
    "# plt.ylabel('SalePrice')\n",
    "ax2.yaxis.label.set_visible(False)\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "sns.regplot(x='YrSold', y='SalePrice', data = test_df_used_for_graphs, label = 'Actual SalePrice', scatter=None, ci=None)\n",
    "sns.regplot(x='YrSold', y='Linear_Regression_Predictions', data = test_df_used_for_graphs, label = 'Predicted SalePrice', scatter=None)\n",
    "plt.title('Feature: YrSold')\n",
    "# plt.ylabel('SalePrice')\n",
    "plt.legend()\n",
    "ax3.set_xticks(list(range(2006, 2011)))\n",
    "ax3.yaxis.label.set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparing predicted SalePrice vs actual SalePrice based on LotArea, MoSold, and YrSold independently for boosted regression tree model\n",
    "test_df_used_for_graphs['Boosted_Regression_Tree_Predictions'] = boosted_trees_model_predictions\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10), nrows=1, ncols=3)\n",
    "fig.suptitle('Boosted Regression Tree Model - Predicted SalePrice vs. Actual SalePrice based on:')\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "sns.regplot(x='LotArea', y='SalePrice', data = test_df_used_for_graphs, label = 'Actual SalePrice', scatter=False, ci=None)\n",
    "sns.regplot(x='LotArea', y='Boosted_Regression_Tree_Predictions', data = test_df_used_for_graphs, label = 'Predicted SalePrice', scatter=False)\n",
    "plt.title('Feature: LotArea')\n",
    "plt.ylabel('SalePrice')\n",
    "plt.legend()\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "sns.regplot(x='MoSold', y='SalePrice', data = test_df_used_for_graphs, label = 'Actual SalePrice', scatter=False, ci=None)\n",
    "sns.regplot(x='MoSold', y='Boosted_Regression_Tree_Predictions', data = test_df_used_for_graphs, label = 'Predicted SalePrice', scatter=False)\n",
    "plt.title('Feature: MoSold')\n",
    "# plt.ylabel('SalePrice')\n",
    "plt.legend()\n",
    "ax2.yaxis.label.set_visible(False)\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "sns.regplot(x='YrSold', y='SalePrice', data = test_df_used_for_graphs, label = 'Actual SalePrice', scatter=None, ci=None)\n",
    "sns.regplot(x='YrSold', y='Linear_Regression_Predictions', data = test_df_used_for_graphs, label = 'Predicted SalePrice', scatter=None)\n",
    "plt.title('Feature: YrSold')\n",
    "# plt.ylabel('SalePrice')\n",
    "plt.legend()\n",
    "ax3.set_xticks(list(range(2006, 2011)))\n",
    "ax3.yaxis.label.set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"Summary\"></a>\n",
    "# Summary\n",
    "[Return to TOC](#Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing RMSE for all Regression Models (Regression Model Performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRfig = plt.figure()\n",
    "BRax = BRfig.add_axes([0,0,1,1])\n",
    "\n",
    "regression_model_labels = ['Generic Regression Model','Ideal Linear Model','Boosted Trees Model']\n",
    "RMSE_Values = [regression_model_errors['rmse'],final_ideal_linear_regression_model_features_and_errors[1],boosted_trees_model_errors]\n",
    "\n",
    "# Plotting bar chart for RMSE of each model after testing evaluation\n",
    "BRax.bar(regression_model_labels,RMSE_Values, color=['purple', 'blue', 'green'])\n",
    "plt.title('Regression models',fontsize=20)\n",
    "plt.xlabel('Models',fontsize=15)\n",
    "plt.ylabel('RMSE',fontsize=15)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing Accuracy for all Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLfig = plt.figure()\n",
    "BLax = BLfig.add_axes([0,0,1,1])\n",
    "\n",
    "classification_model_labels = ['Generic Classification Model','Random Forest Model']\n",
    "accuracy_values = [classifier_model_errors['accuracy'] * 100, random_forest_classifier_errors * 100]\n",
    "\n",
    "# Plotting bar chart for accuracy of each model after testing evaluation\n",
    "BLax.bar(classification_model_labels, accuracy_values, color =['blue','green', 'purple'])\n",
    "plt.title('Classification models',fontsize=20)\n",
    "plt.xlabel('Models',fontsize=15)\n",
    "plt.ylabel('Accuracy (%)',fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
